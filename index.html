<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    heading2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/science.png">
  <title>Jonathan Richard Schwarz</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="90%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Jonathan Richard Schwarz</name>
              </p>
              <p>
              I'm a Senior Research Scientist at <a href="https://deepmind.com">DeepMind</a> and a PhD Candidate at <a href="https://www.ucl.ac.uk/">University College London</a>. I'm advised by <a href="https://www.stats.ox.ac.uk/~teh/">Yee Whye Teh</a> and <a href="http://www.gatsby.ucl.ac.uk/~pel/">Peter Latham</a> and work on problems at the intersection of Probabilistic Modelling, Sparsity and Meta & Continual learning. 
              </p>
              <p>
              A central question of my research is how meaningful prior information from related problems can be learned and then optimally employed to accelerate learning on new tasks, both in terms of data and computationally efficiency. Other interests include compression with Implicit Neural Representations (INRs), Multi-task learning and Bayesian Optimisation. 
              </p>

              <p>
I'm also passionate about outreach & STEM education and frequently participate in events such as <a href="https://www.ucl.ac.uk/">CS Masterclasses at the Royal Institution (2022-)</a>, lectures/mentoring/practicals at the <a href="https://deeplearningindaba.com/2022/">Deep Learning Indaba (2019, 2021, 2022)</a> & <a href="https://aimas.cs.pub.ro/">AIMAS Summer School (2021)</a> as well as tutorials at <a href="https://girlswhocode.com/">Girls who code (2021)</a> and <a href="https://www.blacktechfest.com/">Black Tech Fest (2020)</a>.
              </p>
              <p align=center>
                <a href="mailto:schwarzjn@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=Efs3XxQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/schwarzjn_">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/schwarzjonathan/">LinkedIn</a> &nbsp/&nbsp
                Full CV: <i>on request</i>
              </p>
            </td>
            <td width="33%">
              <img width="120%" src="images/jonathan_schwarz_circle.jpg">
            </td>
          </tr>
        </table>
        
	<heading>News</heading>
	<br>
	<br>
	<ul>
	  <li>Released our new paper on <a href="https://arxiv.org/abs/2301.09479">Modality-agnostic data compression!</a> (Jan 2023).</li>
	  <li>Had a great time visiting <a href="https://www.hku.hk/">The University of Hong Kong</a> and <a href="https://www.hku.hk/">City University of Hong Kong</a> (November 2022).</li>
	  <li>I'm giving a talk at <a href="https://www.cam.ac.uk/">The University of Cambridge</a> on sparsity techniques for Meta & Continual Learning (October 2022).</li>
	  <li><a href="https://arxiv.org/abs/2010.14274"> Behavior priors for efficient reinforcement learning</a> was accepted to JMLR (September 2022)!</li>
	  <li><a href="https://arxiv.org/abs/2205.08957"> Meta-Learning Sparse Compression Networks (MSCN)</a> was accepted to TMLR (August 2022)!</li>
	  <li>I'm giving a talk at <a href="https://www.tue.nl/en/">TU Eindhoven</a> on sparsity techniques for Continual Learning (June 2022).</li>
	  <li>Honoured to be invited to the <a href="https://sites.google.com/view/clvision2022/overview">CVPR 2022 Workshop on CL for Vision</a> to speak about sparsity techniques for Continual Learning and participate at the panel discussion. (June 2022)</li>
	  <li>I'm giving a talk at <a href="https://www.kaist.ac.kr/en/">KAIST</a> on our new paper ``Meta-learning Sparse Compression Networks'' (May 2022)</li>
	  <li>My new paper on Meta-learned Sparsity and compression with INRs is <a href="https://arxiv.org/abs/2205.08957?context=cs.LG">now available!</a></li>
	  <li><a href="https://arxiv.org/abs/2110.00296"> Powerpropagation</a> accepted to NeurIPS 2021 and as a spotlight talk at <a href="https://sites.google.com/view/sparsity-workshop-2021/home">Sparsity in Neural Networks 2021</a>!</li>
	  <li>The <a href="https://meta-learn.github.io/2021/"> Workshop on Meta Learning</a> is back at NeurIPS 2021</li>
	  <li>I'm giving a lecture on Deep Learning with Low Resources @ IndabaX Sudan (September 2021)</li>
	  <li>I'm giving a lecture on Meta & Continual Learning at the Polytechnic University of Bucharest (July 2021)</li>
	  <li><a href="https://arxiv.org/abs/2110.00296"> Powerpropagation</a> was accepted as a Spotlight talk at the Sparsity in Neural Networks 2021 Workshop!</li>
	  <li>We are organising a (virtual) <a href="https://meta-learn.github.io/2020/"> Workshop on Meta Learning</a> at NeurIPS 2020</li>
	  <li>Talk on Continual Learning @ Kheiron Medical (June 2020)</li>
	  <li>We are organising a (virtual) <a href="https://sites.google.com/view/cl-icml"> Workshop on Continual Learning</a> at ICML 2020</li>
	  <li><a href="https://arxiv.org/abs/1901.11356">Functional regularisation for continual learning</a> got accepted to ICLR 2020!</li>
	  <li><a href="https://openreview.net/forum?id=rylnK6VtDH">Multiplicative Interactions and Where to Find Them</a> got accepted to ICLR 2020!</li>
	</ul>
	<br>
	<br>
	
        <heading>Research</heading>
	<br>
	<br>
	<center>
	Selected papers are <span class="highlight">highlighted</span>.

	<br>
	<heading2>All papers</heading2>
	<br>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/vc_inr.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2301.09479">
                  <papertitle>Modality-Agnostic Variational Compression of Implicit Neural Representations (VC-INR)</papertitle>
                </a>
                <br>
                <strong>Jonathan Richard Schwarz</strong>*, Jihoon Tack*, Yee Whye Teh, Jaeho Lee, Jinwoo Shin
		<br>
	  	<br>
		<em>arXiv 2023</em>
		<br>
		<br>
	  	<em>* : Joint first authorship</em>
              </p>
          </td>
        </table>
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/mscn.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2205.08957">
                  <papertitle>Meta-Learning Sparse Compression Networks (MSCN)</papertitle>
                </a>
                <br>
                <strong>Jonathan Schwarz</strong>, Yee Whye Teh
                <br>
                <br>
                <em>Transactions on Machine Learning Research (TMLR) 2022</em>
                <br>
                <br>
              </p>
          </td>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/bpriors.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2010.14274">
                  <papertitle>Behavior Priors for Efficient Reinforcement Learning</papertitle>
                </a>
                <br>
		Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, <strong>Jonathan Schwarz</strong>, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, Nicolas Heess
                <br>
                <br>
                <em>Journal of Machine Learning Research (JMLR) 2022</em>
                <br>
                <br>
              </p>
          </td>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/powerprop.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/2110.00296">
                    <papertitle>Powerpropagation: A sparsity inducing weight reparameterisation</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>, Siddhant M. Jayakumar, Razvan Pascanu, Peter E. Latham, Yee Whye Teh
                  <br>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS) 2021</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/deepmind-research/tree/master/powerpropagation">Code</a>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/gp.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1901.11356">
                    <papertitle>Functional Regularisation for Continual Learning using Gaussian Processes</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>*, Michalis K. Titsias*, Alexander G. de G. Matthews, Razvan Pascanu, Yee Whye Teh
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2020</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/deepmind-research/blob/master/functional_regularisation_for_continual_learning/frcl.ipynb">Code</a>
		  <br>
                  <br>
                  <em>* : Joint first authorship</em>
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/multi.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://openreview.net/forum?id=rylnK6VtDH">
                    <papertitle>Multiplicative Interactions and Where to Find Them</papertitle>
                  </a>
                  <br>
                  Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, <strong>Jonathan Schwarz</strong>, Jack Rae, Simon Osindero, Yee Whye Teh, Tim Harley, Razvan Pascanu
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2020</em>
                  <br>
                  <br>
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/np_model_based.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="http://arxiv.org/abs/1903.11907">
                  <papertitle>Meta-Learning surrogate models for sequential decision making</papertitle>
                </a>
                <br>
                  <strong>Jonathan Schwarz</strong>*, Alexandre Galashov*, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet Kohli, SM Ali Eslami°, Yee Whye Teh°
                <br>
                <br>
                <em>ICLR 2019 Workshop on Structure & Priors in Reinforcement Learning</em>
                <br>
                <br>
                <em>*, ° : Joint first/senior authorship</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/storage.png" alt="clean-usnob" width="140" height="120",  class="center"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1811.11682">
                    <papertitle>Experience replay for continual learning</papertitle>
                  </a>
                  <br>
                  David Rolnick, Arun Ahuja, <strong>Jonathan Schwarz</strong>, Timothy P. Lillicrap, Greg Wayne
                  <br>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS) 2019</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/kl.png" alt="clean-usnob" width="160" height="130"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1905.01240">
                    <papertitle>Information asymmetry in KL-regularized RL</papertitle>
                  </a>
                  <br>
                  Alexandre Galashov, Siddhant M Jayakumar, Leonard Hasenclever, Dhruva Tirumala, <strong>Jonathan Schwarz</strong>, Guillaume Desjardins, Wojciech M Czarnecki, Yee Whye Teh, Razvan Pascanu, Nicolas Heess
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2019</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/comparison.png" alt="clean-usnob" width="160" height="130"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="http://bayesiandeeplearning.org/2018/papers/92.pdf">
                    <papertitle>Empirical Evaluation of Neural Process Objectives</papertitle>
                  </a>
                  <br>
                  Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, <strong>Jonathan Schwarz</strong>, Yee Whye Teh
                  <br>
                  <br>
                  <em>NeurIPS 2018 workshop on Bayesian Deep Learning</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/anp.png" alt="clean-usnob" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1901.05761">
                    <papertitle>Attentive Neural Processes</papertitle>
                  </a>
                  <br>
                  Hyunjik Kim, Andriy Mnih, <strong>Jonathan Schwarz</strong>, Marta Garnelo, SM Ali Eslami, Dan Rosenbaum, Oriol Vinyals, Yee Whye Teh
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2019</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/neural-processes">Code</a>
                </p>
            </td>
          </tr>
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/np.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1807.01622">
                    <papertitle>Neural Processes</papertitle>
                  </a>
                  <br>
                  Marta Garnelo, <strong>Jonathan Schwarz</strong>, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, Yee Whye Teh
                  <br>
                  <br>
                  <em>ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models</em> <font color="red"><strong> (Spotlight talk)</strong></font>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/neural-processes">Code</a> / <a href="https://www.youtube.com/watch?v=bpsoGt-NYYk">Talk</a> (credit to Marta)
                </p>
            </td>
          </tr>
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/p_and_c.png" alt="clean-usnob" width="160" height="120"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1805.06370">
                    <papertitle>Progress & Compress: A scalable framework for continual learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Raia Hadsell°, Razvan Pascanu°
                  <br>
                  <br>
                  <em>International Conference on Machine Learning (ICML) 2018</em> <font color="red"><strong> (Long oral Presentation)</strong></font>
                  <br>
                  <br>
                  <a href="https://drive.google.com/drive/folders/1CWDGnf_a5YIJH1a9sUNdQsYtRxal4nxf?usp=sharing">Sequential Omniglot Dataset</a> / <a href="https://www.facebook.com/icml.imls/videos/session-3-deep-learning-neural-network-architectures/432573817257139">Talk</a>
                  <br>
                  <br>
                  <em>° : Joint senior authorship</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/narrative_qa.png" alt="clean-usnob" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1712.07040">
                    <papertitle>The NarrativeQA Reading Comprehension Challenge</papertitle>
                  </a>
                  <br>
                  Tomas Kocisky, <strong>Jonathan Schwarz</strong>, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, Edward Grefenstette
                  <br>
                  <br>
                  <em>Transactions of the Association for Computational Linguistics (TACL) 2018</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/narrativeqa">Dataset</a> / <a href="https://vimeo.com/285804931">Talk</a> (credit to Tomas)
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/recurrent_vae.png" alt="clean-usnob" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="http://www.ipab.inf.ed.ac.uk/cgvu/0414.pdf">
                    <papertitle>A Recurrent Variational Autoencoder for Human Motion Synthesis</papertitle>
                  </a>
                  <br>
                  Ikhsanul Habibie, Daniel Holden, <strong>Jonathan Schwarz</strong>, Joe Yearsley, Taku Komura
                  <br>
                  <br>
                  <em>British Machine Vision Conference (BMVC) 2017</em>
                  <br>
                  <br>
		  <a href="https://github.com/Brimborough/deep-motion-analysis">Code</a> / <a href="https://bitbucket.org/jonathan-schwarz/edinburgh_locomotion_mocap_dataset">Dataset</a>
                </p>
            </td>
          </tr>
        </table>

	<br>
	<heading2>Service</heading2>
	<br>
	<br>
	<br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/neurips.jpeg" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://meta-learn.github.io/2021/">
                    <papertitle>NeurIPS 2021 Workshop on Meta Learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>, Fábio Ferreira, Erin Grant, Frank Hutter, Joaquin Vanschoren, Huaxiu Yao
                  <br>
                  <br>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/neurips.jpeg" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://meta-learn.github.io/2020/">
                    <papertitle>NeurIPS 2020 Workshop on Meta Learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>, Roberto Calandra , Jeff Clune , Erin Grant, Joaquin Vanschoren, Francesco Visin, Jane Wang
                  <br>
                  <br>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/icml_cl.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://sites.google.com/view/cl-icml/">
                    <papertitle>ICML 2020 Workshop on Continual Learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Schwarz</strong>, Rahaf Aljundi, Eugene Belilovsky, Arslan Chaudhry, Puneet Dokania, Sayna Ebrahimi, Haytham Fayek, David Lopez-Paz , Marc Pickett
                  <br>
                  <br>
                </p>
            </td>
        </table>

  Based on <a href="https://jonbarron.info/">Jon Barron's</a> website.

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-62553068-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
