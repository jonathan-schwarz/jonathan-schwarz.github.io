<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    heading2 {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 18px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/science.png">
  <title>Jonathan Richard Schwarz</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="90%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Jonathan Richard Schwarz</name>
              </p>              <p>
              I'm a Research Fellow at <a href="https://harvard.edu/">Harvard University</a>, working on all forms of Efficient Machine Learning and its application to scientific and medical problems. Previously, I was a Senior Research Scientist at <a href="https://deepmind.google/">Google DeepMind</a> and obtained my PhD from the joint DeepMind-<a href="https://www.ucl.ac.uk/">University College London</a> programme, advised by <a href="https://www.stats.ox.ac.uk/~teh/">Yee Whye Teh</a> and <a href="http://www.gatsby.ucl.ac.uk/~pel/">Peter Latham</a>. My
              thesis focused on using sparse parameterisations and knowledge transfer for this purpose. Before that, I spent two years at the <a href="https://www.ucl.ac.uk/gatsby/gatsby-computational-neuroscience-unit/">Gatsby Computational Neuroscience Unit</a> and graduated top-of-the class from <a href="https://www.ed.ac.uk/informatics">The University of Edinburgh</a>.
              </p>
              <p>
              My research focuses on the objective of building (i) <b>efficient</b>, (ii) <b>general</b> and (iii) <b>robust</b> Machine Learning systems. A central paradigm in my approach is the design of algorithms that can effectively abstract knowledge and skills present in related problems, enabling their utilisation for efficient learning on future tasks. In this way, agents gradually build diverse repertoires of skills, allowing transfer to future tasks using only a fraction of the otherwise required learning time and/or data. To that end, most of my existing work falls within one or more of the following categories:

                <ul>
                  <li>Meta-Learning:
                        <a href="https://arxiv.org/abs/1807.01622">[ICML WS'18]</a>
                        <a href="https://bayesiandeeplearning.org/2018/papers/92.pdf">[NeurIPS WS'18]</a>
                        <a href="https://arxiv.org/abs/1901.05761">[ICLR'19]</a>
                        <a href="https://arxiv.org/abs/1903.11907">[arXiv'19]</a>
                        <a href="https://arxiv.org/abs/2205.08957">[TMLR'22]</a>
                        <a href="https://arxiv.org/abs/2301.09479">[ICML'23]</a>
                        <a href="https://arxiv.org/abs/2302.00617">[NeurIPS'23a]</a>
                        <a href="https://openreview.net/pdf?id=tt7bQnTdRm">[NeurIPS'23b]</a>
                        <a href="https://arxiv.org/abs/2403.08477">[arXiv'24]</a>
                  </li>
                  <li>Sparsity & Efficient Parameterizations:
                        <a href="https://arxiv.org/abs/1905.01240">[ICLR'19]</a>
                        <a href="https://openreview.net/forum?id=rylnK6VtDH">[ICLR'20]</a>
                        <a href="https://arxiv.org/abs/2110.00296">[NeurIPS'21]</a>
                        <a href="https://arxiv.org/abs/2205.08957">[TMLR'22]</a>
                        <a href="https://arxiv.org/abs/2010.14274">[JMLR'22]</a>
                        <a href="https://arxiv.org/abs/2301.09479">[ICML'23]</a></li>
                        <a href="https://arxiv.org/abs/2403.08477">[arXiv'24]</a>
                  <li>INRs / Neural Data Compression:
                        <a href="https://arxiv.org/abs/2205.08957">[TMLR'22]</a>
                        <a href="https://arxiv.org/abs/2301.09479">[ICML'23]</a></li>
                        <a href="https://arxiv.org/abs/2302.03130">[ICLR WS'23]</a>
                        <a href="https://arxiv.org/abs/2302.00617">[NeurIPS'23]</a>
                        <a href="https://arxiv.org/abs/2312.02753v1">[CVPR'24]</a>
                  <li>Data Pruning / Online Curriculum Learning:
                        <a href="https://arxiv.org/abs/2302.00617">[NeurIPS'23]</a>
                        <a href="https://arxiv.org/abs/2312.05328">[arXiv'23]</a>
                  <li>Continual Learning:
                        <a href="https://arxiv.org/abs/1805.06370">[ICML'18]</a>
                        <a href="https://arxiv.org/abs/1811.11682">[NeurIPS'19]</a>
                        <a href="https://arxiv.org/abs/1901.11356">[ICLR'20]</a>
                        <a href="https://arxiv.org/abs/2110.00296">[NeurIPS'21]</a>
                        <a href="https://arxiv.org/abs/2403.04317">[arXiv'24]</a>
                  </li>
                </ul>
              </p>

              <table width="100%" border="0" cellspacing="0" cellpadding="20">
              <tr bgcolor="#ffffd0">
	      </td>
              </table>

              <table width="100%" border="0" cellspacing="0" cellpadding="20">
		<td width="25%"><img src="images/h_logo.jpeg" alt="clean-usnob" width="15" height="15">
		      I'm also the lead organiser of the Harvard Efficient Machine Learning Seminar Series. Join us <a href="http://efficientml.org/">here</a> üìà.
              </table>

              </p>
              <p align=center>
                <a href="mailto:schwarzjn@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=Efs3XxQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/schwarzjn_">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/schwarzjonathan/">LinkedIn</a> &nbsp/&nbsp
                Full CV: <i>on request</i>
              </p>
            </td>
            <td width="33%">
              <img width="120%" src="images/profile.jpeg">
            </td>
          </tr>
        </table>
        
	<center>
	<heading>News</heading>
	</center>
	<br>
	<ul>
	  <li>Invited talk at the Hong Kong University of Science and Technology (<a href="https://hkust.edu.hk/">HKUST</a>) (April 2024).</a></li>
	  <li>Releasing our new perspective paper on <a href="https://arxiv.org/abs/2404.02831">Empowering Scientific Discovery with AI Agents</a> (April 2024).</a></li>
	  <li>Releasing <a href="https://arxiv.org/abs/2403.08477">SMAT</a>, our new state-of-the-art Meta-Learner (March 2024) !</a></li>
	  <li>Announcing the <a href="https://sites.google.com/view/harvard-efficient-ml"> Harvard Efficient ML Seminar Series</a> (March 2024) !</a></li>
	  <li>Announcing <a href="https://arxiv.org/abs/2403.04317">MAC</a>, our new RAG/Online Adaptation method for LLMs (March 2024).</li>
	  <li><a href="https://arxiv.org/abs/2312.02753v1">C3</a> accepted to CVPR (March 2024).</li>
	  <li>Invited talk at MIT on continual learning for LLMs (February 2024).</li>
	  <li>Invited talk at the <a href="https://web.stanford.edu/group/it-forum/colloquium.html">Stanford Information Theory Forum</a> on neural data compression with INRs (January 2024).</a></li>
	  <li>Invited talk at <a href="https://mistral.ai/">Mistral</a> on <a href="https://arxiv.org/abs/2312.05328">Bad Students Make Great Teachers</a> (January 2024).</a></li>
	  <li>Realeasing our new <a href="https://arxiv.org/abs/2312.05328">distributed</a> curriculum learning framework (December 2023).</a></li>
	  <li>Introducing <a href="https://arxiv.org/abs/2312.02753v1">C3</a>, our latest neural compression method (December 2023).</a></li>
	  <li>Invited talk at Stanford University (December 2023).</a></li>
	  <li>I've started a new position at Harvard, bringing advances in efficient ML to science and medicine !</a></li>
	  <li>Serving as area chair for ICLR'24.</a></li>
	  <li>Two papers accepted at NeurIPS 2023 !</a></li>
	  <li>Invited talk at The Royal Institution's Youth Summit on "AI for Scientific Progress" (September 2023).</a></li>
	  <li>Invited talk at Havard University on "Towards efficient and robust Machine Learning" (June 2023).</a></li>
	  <li>I'm visiting ETH Zurich to give a talk on Neural data compression with INRs (May 2023).</a></li>
	  <li>Join me and my co-organisers at the <a href="https://sites.google.com/view/neural-fields/">ICLR 2023 workshop on Neural Fields</a></li>
	  <li><a href="https://arxiv.org/abs/2301.09479">VC-INR</a> accepted to ICML 2023 !</li>
	  <li>Released our new paper on <a href="https://arxiv.org/abs/2301.09479">Modality-agnostic data compression !</a> (Feb 2023).</li>
	  <li>We have a new paper on <a href="https://arxiv.org/abs/2302.00617">Efficient Meta-Learning for large context sets.</a> (Feb 2023).</li>
	  <li>New work on <a href="https://arxiv.org/abs/2302.03130">Spatial Functa</a> ! (Feb 2023).</li>
	  <li>Had a great time visiting <a href="https://www.hku.hk/">The University of Hong Kong</a> and <a href="https://www.hku.hk/">City University of Hong Kong</a> (November 2022).</li>
	  <li>I'm giving a talk at <a href="https://www.cam.ac.uk/">The University of Cambridge</a> on sparsity techniques for Meta & Continual Learning (October 2022).</li>
	  <li><a href="https://arxiv.org/abs/2010.14274"> Behavior priors for efficient reinforcement learning</a> was accepted to JMLR (September 2022) !</li>
	  <li><a href="https://arxiv.org/abs/2205.08957"> Meta-Learning Sparse Compression Networks (MSCN)</a> was accepted to TMLR (August 2022) !</li>
	  <li>I'm giving a talk at <a href="https://www.tue.nl/en/">TU Eindhoven</a> on sparsity techniques for Continual Learning (June 2022).</li>
	  <li>Honoured to be invited to the <a href="https://sites.google.com/view/clvision2022/overview">CVPR 2022 Workshop on CL for Vision</a> to speak about sparsity techniques for Continual Learning and participate at the panel discussion. (June 2022)</li>
	  <li>I'm giving a talk at <a href="https://www.kaist.ac.kr/en/">KAIST</a> on our new paper "Meta-learning Sparse Compression Networks" (May 2022)</li>
	  <li>My new paper on Meta-learned Sparsity and compression with INRs is <a href="https://arxiv.org/abs/2205.08957?context=cs.LG">now available !</a></li>
	</ul>

	<br>
	<br>
	<center>
	<heading>Research</heading>
	<br>
	<br>
	Selected papers are <span class="highlight">highlighted</span>.
	</center>
	<br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr bgcolor="#ffffd0">
          <td width="25%"><img src="images/smat.png" alt="clean-usnob" width="160" height="120"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2403.08477">
                  <papertitle>Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</papertitle>
                </a>
                <br>
                Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Ying Wei¬∞, <strong>Jonathan Richard Schwarz¬∞</strong>
		<br>
	  	<br>
                <a href="https://github.com/szc12153/sparse_meta_tuning">üíª Code</a>
	  	<br>
                <a href="https://szc12153.github.io/">üîó Project Website</a>
                <br>
                <br>
		<em>arXiv 2024</em>
                <br>
                <br>
                <em>¬∞ : Joint senior authorship</em>
              </p>
          </td>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/bio_agents.png" alt="clean-usnob" width="160" height="120"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2404.02831">
                  <papertitle>Empowering Biomedical Discovery with AI Agents</papertitle>
                </a>
                <br>
                Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, <strong>Jonathan Richard Schwarz</strong>, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik
                <br>
                <br>
		<em>arXiv 2024</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/mac.png" alt="clean-usnob" width="160" height="90"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2403.04317">
                  <papertitle>Online Adaptation of Language Models with a Memory of Amortized Contexts</papertitle>
                </a>
                <br>
                Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, <strong>Jonathan Richard Schwarz</strong>
		<br>
	  	<br>
                <a href="https://jihoontack.github.io/assets/mac.pdf">üó£Ô∏è Slides</a>
	  	<br>
                <a href="https://www.youtube.com/watch?v=7WG6-aYBgX4">üéûÔ∏è Video</a>
	  	<br>
                <a href="https://github.com/jihoontack/MAC">üíª Code</a>
	  	<br>
                <a href="https://jihoontack.github.io/MAC/">üîó Project Website</a>
                <br>
                <br>
		<em>arXiv 2024</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr bgcolor="#ffffd0">
          <td width="25%"><img src="images/active_learning.png" alt="clean-usnob" width="160" height="130"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2312.05328">
                  <papertitle>Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding</papertitle>
                </a>
                <br>
                Talfan Evans, Shreya Pathak, Hamza Merzic, <strong>Jonathan Richard Schwarz</strong>, Ryutaro Tanno, Olivier J. Henaff
		<br>
	  	<br>
		<em>arXiv 2023</em>
              </p>
          </td>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/c3.png" alt="clean-usnob" width="160" height="120"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2312.02753v1">
                  <papertitle>C3: High-performance and low-complexity neural compression from a single image or video</papertitle>
                </a>
                <br>
                Hyunjik Kim, Matthias Bauer, Lucas Theis, <strong>Jonathan Richard Schwarz</strong>, Emilien Dupont
		<br>
	  	<br>
                <a href="https://github.com/google-deepmind/c3_neural_compression">üíª Code</a>
	  	<br>
                <a href="https://c3-neural-compression.github.io/">üîó Project Website</a>
                <br>
                <br>
		<em>CVPR 2024</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/ecop.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2302.00617">
                  <papertitle>Efficient Meta-Learning via Error-based Context Pruning for Implicit Neural Representations</papertitle>
                </a>
                <br>
                Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, <strong>Jonathan Richard Schwarz</strong>
		<br>
	  	<br>
                <a href="https://jihoontack.github.io/assets/gradncp.pdf">üó£Ô∏è Slides</a>
	  	<br>
                <a href="https://github.com/jihoontack/GradNCP">üíª Code</a>
		<br>
	  	<br>
		<em>NeurIPS 2023</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/ebml.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://openreview.net/pdf?id=tt7bQnTdRm">
                  <papertitle>Secure Out-of-Distribution Task Generalization with Energy-Based Models</papertitle>
                </a>
                <br>
                Shengzhuang Chen, Long-Kai Huang, <strong>Jonathan Richard Schwarz</strong>, Yilun Du, Ying Wei
		<br>
	  	<br>
		<em>NeurIPS 2023</em>
              </p>
          </td>
        </table>
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr bgcolor="#ffffd0">
          <td width="25%"><img src="images/vc_inr.png" alt="clean-usnob" width="160" height="160"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2301.09479">
                  <papertitle>Modality-Agnostic Variational Compression of Implicit Neural Representations (VC-INR)</papertitle>
                </a>
                <br>
                <strong>Jonathan Richard Schwarz</strong>*, Jihoon Tack*, Yee Whye Teh, Jaeho Lee, Jinwoo Shin
		<br>
	  	<br>
		<em>ICML 2023</em>
		<br>
		<br>
	  	<em>* : Joint first authorship</em>
              </p>
          </td>
          </td>
        </table>
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/spatial_functa.png" alt="clean-usnob" width="160" height="180"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2302.03130">
                  <papertitle>Spatial Functa: Scaling Functa to ImageNet Classification and Generation</papertitle>
                </a>
                <br>
		Matthias Bauer*, Emilien Dupont, Andy Brock, Dan Rosenbaum, <strong>Jonathan Richard Schwarz</strong>, Hyunjik Kim*
		<br>
	  	<br>
		<em>arXiv 2023</em>
		<br>
		<br>
              </p>
          </td>
        </table>	
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/mscn.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2205.08957">
                  <papertitle>Meta-Learning Sparse Compression Networks (MSCN)</papertitle>
                </a>
                <br>
                <strong>Jonathan Richard Schwarz</strong>, Yee Whye Teh
                <br>
                <br>
                <em>Transactions on Machine Learning Research (TMLR) 2022</em>
                <br>
                <br>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/bpriors.png" alt="clean-usnob" width="160" height="120"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="https://arxiv.org/abs/2010.14274">
                  <papertitle>Behavior Priors for Efficient Reinforcement Learning</papertitle>
                </a>
                <br>
		Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan Pascanu, <strong>Jonathan Richard Schwarz</strong>, Guillaume Desjardins, Wojciech Marian Czarnecki, Arun Ahuja, Yee Whye Teh, Nicolas Heess
                <br>
                <br>
                <em>Journal of Machine Learning Research (JMLR) 2022</em>
                <br>
                <br>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/powerprop.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/2110.00296">
                    <papertitle>Powerpropagation: A sparsity inducing weight reparameterisation</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Richard Schwarz</strong>, Siddhant M. Jayakumar, Razvan Pascanu, Peter E. Latham, Yee Whye Teh
                  <br>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS) 2021</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/deepmind-research/tree/master/powerpropagation">üíª Code</a>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/gp.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1901.11356">
                    <papertitle>Functional Regularisation for Continual Learning using Gaussian Processes</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Richard Schwarz</strong>*, Michalis K. Titsias*, Alexander G. de G. Matthews, Razvan Pascanu, Yee Whye Teh
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2020</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/deepmind-research/blob/master/functional_regularisation_for_continual_learning/frcl.ipynb">üíª Code</a>
		  <br>
                  <br>
                  <em>* : Joint first authorship</em>
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/multi.png" alt="clean-usnob" width="160" height="100"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://openreview.net/forum?id=rylnK6VtDH">
                    <papertitle>Multiplicative Interactions and Where to Find Them</papertitle>
                  </a>
                  <br>
                  Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, <strong>Jonathan Richard Schwarz</strong>, Jack Rae, Simon Osindero, Yee Whye Teh, Tim Harley, Razvan Pascanu
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2020</em>
                  <br>
                  <br>
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="25%"><img src="images/np_model_based.png" alt="clean-usnob" width="160" height="140"></td>
          <td width="75%" valign="top">
            <p>
              <p>
                <a href="http://arxiv.org/abs/1903.11907">
                  <papertitle>Meta-Learning surrogate models for sequential decision making</papertitle>
                </a>
                <br>
                  <strong>Jonathan Richard Schwarz</strong>*, Alexandre Galashov*, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet Kohli, SM Ali Eslami¬∞, Yee Whye Teh¬∞
                <br>
                <br>
                <em>ICLR 2019 Workshop on Structure & Priors in Reinforcement Learning</em>
                <br>
                <br>
                <em>*, ¬∞ : Joint first/senior authorship</em>
              </p>
          </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%"><img src="images/storage.png" alt="clean-usnob" width="140" height="120",  class="center"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1811.11682">
                    <papertitle>Experience replay for continual learning</papertitle>
                  </a>
                  <br>
                  David Rolnick, Arun Ahuja, <strong>Jonathan Richard Schwarz</strong>, Timothy P. Lillicrap, Greg Wayne
                  <br>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS) 2019</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/kl.png" alt="clean-usnob" width="160" height="110"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1905.01240">
                    <papertitle>Information asymmetry in KL-regularized RL</papertitle>
                  </a>
                  <br>
                  Alexandre Galashov, Siddhant M Jayakumar, Leonard Hasenclever, Dhruva Tirumala, <strong>Jonathan Richard Schwarz</strong>, Guillaume Desjardins, Wojciech M Czarnecki, Yee Whye Teh, Razvan Pascanu, Nicolas Heess
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2019</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/comparison.png" alt="clean-usnob" width="160" height="130"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="http://bayesiandeeplearning.org/2018/papers/92.pdf">
                    <papertitle>Empirical Evaluation of Neural Process Objectives</papertitle>
                  </a>
                  <br>
                  Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, <strong>Jonathan Richard Schwarz</strong>, Yee Whye Teh
                  <br>
                  <br>
                  <em>NeurIPS 2018 workshop on Bayesian Deep Learning</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/anp.png" alt="clean-usnob" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1901.05761">
                    <papertitle>Attentive Neural Processes</papertitle>
                  </a>
                  <br>
                  Hyunjik Kim, Andriy Mnih, <strong>Jonathan Richard Schwarz</strong>, Marta Garnelo, SM Ali Eslami, Dan Rosenbaum, Oriol Vinyals, Yee Whye Teh
                  <br>
                  <br>
                  <em>International Conference on Learning Representations (ICLR) 2019</em>
                  <br>
                  <br>
                  <a href="https://github.com/deepmind/neural-processes">üíª Code</a>
                </p>
            </td>
          </tr>
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/np.png" alt="clean-usnob" width="160" height="140"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1807.01622">
                    <papertitle>Neural Processes</papertitle>
                  </a>
                  <br>
                  Marta Garnelo, <strong>Jonathan Richard Schwarz</strong>, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, Yee Whye Teh
                  <br>
                  <br>
                  <em>ICML 2018 Workshop on Theoretical Foundations and Applications of Deep Generative Models</em> <font color="red"><strong> (Spotlight talk)</strong></font>
                  <br>
                  <br>
		  <a href="https://www.youtube.com/watch?v=bpsoGt-NYYk">üó£Ô∏è Talk (credit to Marta)</a>
                  <br>
                  <a href="https://github.com/deepmind/neural-processes">üíª Code</a>
                </p>
            </td>
          </tr>
          <tr bgcolor="#ffffd0">
            <td width="25%"><img src="images/p_and_c.png" alt="clean-usnob" width="160" height="120"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1805.06370">
                    <papertitle>Progress & Compress: A scalable framework for continual learning</papertitle>
                  </a>
                  <br>
                  <strong>Jonathan Richard Schwarz</strong>, Jelena Luketina, Wojciech M. Czarnecki, Agnieszka Grabska-Barwinska, Yee Whye Teh, Raia Hadsell¬∞, Razvan Pascanu¬∞
                  <br>
                  <br>
                  <em>International Conference on Machine Learning (ICML) 2018</em> <font color="red"><strong> (Long oral)</strong></font>
                  <br>
                  <br>
		  <a href="https://www.facebook.com/icml.imls/videos/session-3-deep-learning-neural-network-architectures/432573817257139">üó£Ô∏è Talk</a>
                  <br>
		  <a href="https://drive.google.com/drive/folders/1CWDGnf_a5YIJH1a9sUNdQsYtRxal4nxf?usp=sharing">üìä Data (Sequential Omniglot)</a>
                  <br>
                  <br>
                  <em>¬∞ : Joint senior authorship</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/narrative_qa.png" alt="clean-usnob" width="160" height="160"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="https://arxiv.org/abs/1712.07040">
                    <papertitle>The NarrativeQA Reading Comprehension Challenge</papertitle>
                  </a>
                  <br>
                  Tomas Kocisky, <strong>Jonathan Richard Schwarz</strong>, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, Edward Grefenstette
                  <br>
                  <br>
		  <a href="https://vimeo.com/285804931">üó£Ô∏è Talk (credit to Tomas)</a>
                  <br>
		  <a href="https://github.com/deepmind/narrativeqa">üìä Data</a>
                  <br>
                  <br>
                  <em>Transactions of the Association for Computational Linguistics (TACL) 2018</em>
                </p>
            </td>
          </tr>
          <tr>
            <td width="25%"><img src="images/recurrent_vae.png" alt="clean-usnob" width="160" height="120"></td>
            <td width="75%" valign="top">
              <p>
                <p>
                  <a href="http://www.ipab.inf.ed.ac.uk/cgvu/0414.pdf">
                    <papertitle>A Recurrent Variational Autoencoder for Human Motion Synthesis</papertitle>
                  </a>
                  <br>
                  Ikhsanul Habibie, Daniel Holden, <strong>Jonathan Richard Schwarz</strong>, Joe Yearsley, Taku Komura
                  <br>
                  <br>
		  <a href="https://github.com/Brimborough/deep-motion-analysis">üíª Code</a>
                  <br>
		  <a href="https://bitbucket.org/jonathan-schwarz/edinburgh_locomotion_mocap_dataset">üìä Data</a>
                  <br>
                  <br>
                  <em>British Machine Vision Conference (BMVC) 2017</em>
                </p>
            </td>
          </tr>
        </table>

	<br>
	<center>
	<heading>Academic Workshops</heading>
	</center>
	<br>
	<br>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="15%"><img src="images/neural_fields.png" alt="clean-usnob" width="80" height="60"></td>
            <td width="85%" valign="top">
              <p>
                <p>
                  <a href="https://sites.google.com/view/neural-fields">
                    <papertitle>(ICLR 2023) Neural Fields across Fields: Methods and Applications of Implicit Neural Representations</papertitle>
                  </a>
                  <br>
		  <br>
                  <strong>Jonathan Richard Schwarz</strong>, Hyunjik Kim, Emilien Dupont, Thu Nguyen-Phuoc, Vincent Sitzman, Srinath Sridhar
                  <br>
                  <br>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="15%"><img src="images/neurips.jpeg" alt="clean-usnob" width="80" height="80"></td>
            <td width="85%" valign="top">
              <p>
                <p>
                  <a href="https://meta-learn.github.io/2021/">
                    <papertitle>NeurIPS 2021 Workshop on Meta Learning</papertitle>
                  </a>
                  <br>
		  <br>
                  <strong>Jonathan Richard Schwarz</strong>, F√°bio Ferreira, Erin Grant, Frank Hutter, Joaquin Vanschoren, Huaxiu Yao
                  <br>
                  <br>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="15%"><img src="images/neurips.jpeg" alt="clean-usnob" width="80" height="80"></td>
            <td width="85%" valign="top">
              <p>
                <p>
                  <a href="https://meta-learn.github.io/2020/">
                    <papertitle>NeurIPS 2020 Workshop on Meta Learning</papertitle>
                  </a>
                  <br>
		  <br>
                  <strong>Jonathan Richard Schwarz</strong>, Roberto Calandra, Jeff Clune, Erin Grant, Joaquin Vanschoren, Francesco Visin, Jane Wang
                  <br>
                  <br>
                </p>
            </td>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="15%"><img src="images/icml_cl.png" alt="clean-usnob" width="80" height="80"></td>
            <td width="85%" valign="top">
              <p>
                <p>
                  <a href="https://sites.google.com/view/cl-icml/">
                    <papertitle>ICML 2020 Workshop on Continual Learning</papertitle>
                  </a>
                  <br>
		  <br>
                  <strong>Jonathan Richard Schwarz</strong>, Rahaf Aljundi, Eugene Belilovsky, Arslan Chaudhry, Puneet Dokania, Sayna Ebrahimi, Haytham Fayek, David Lopez-Paz , Marc Pickett
                  <br>
                  <br>
                </p>
            </td>
        </table>

  Based on <a href="https://jonbarron.info/">Jon Barron's</a> website.

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-62553068-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->

</body>

</html>
